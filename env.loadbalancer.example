# Load Balancer Configuration Example
# Copy this to .env and configure your API keys and endpoints

# Multiple OpenAI API Keys (comma-separated)
# The load balancer will cycle through these keys in round-robin fashion
OPENAI_API_KEYS="sk-openai-key-1,sk-openai-key-2,sk-openai-key-3"

# Multiple Anthropic API Keys (comma-separated)
# The load balancer will cycle through these keys in round-robin fashion
ANTHROPIC_API_KEYS="sk-ant-api-key-1,sk-ant-api-key-2"

# Optional: Gemini API Key (for Google models)
GEMINI_API_KEY="your-gemini-api-key"

# Service Endpoints Configuration (JSON format)
# Maps service names to arrays of endpoint URLs
# The load balancer will cycle through endpoints for each service
SERVICE_ENDPOINTS={
  "openai": [
    "https://api.openai.com/v1",
    "https://your-openai-proxy-1.com/v1",
    "https://your-openai-proxy-2.com/v1"
  ],
  "anthropic": [
    "https://api.anthropic.com/v1",
    "https://your-anthropic-proxy.com/v1"
  ]
}

# Original proxy configuration (still supported for compatibility)
PREFERRED_PROVIDER="openai"
BIG_MODEL="gpt-4.1"
SMALL_MODEL="gpt-4.1-mini"

# Alternative configurations:

# Example 1: Simple single key setup (fallback to original behavior)
# OPENAI_API_KEY="sk-your-single-openai-key"
# ANTHROPIC_API_KEY="sk-your-single-anthropic-key"

# Example 2: Multiple keys with default endpoints
# OPENAI_API_KEYS="key1,key2,key3"
# ANTHROPIC_API_KEYS="key1,key2"
# # SERVICE_ENDPOINTS will default to official APIs

# Example 3: Custom endpoint setup with multiple services
# SERVICE_ENDPOINTS={
#   "openai": [
#     "https://api.openai.com/v1",
#     "https://your-custom-openai-endpoint.com/v1",
#     "https://another-openai-proxy.com/v1"
#   ],
#   "anthropic": [
#     "https://api.anthropic.com/v1",
#     "https://your-anthropic-gateway.com/v1"
#   ]
# } 
